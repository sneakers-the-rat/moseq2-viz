'''
Functions for creating fingerprint plots and linear classifier
'''
from operator import pos
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from collections import defaultdict
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler

from sklearn.model_selection import StratifiedKFold, LeaveOneOut, RepeatedStratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
from sklearn.base import clone

import pandas as pd
import numpy as np


def robust_min(v):
    return v.quantile(0.01)


def robust_max(v):
    return v.quantile(0.99)


def _apply_to_col(df, fn, **kwargs):
    return df.apply(fn, axis=0, **kwargs)


def create_fingerprint_dataframe(scalar_df, mean_df, n_bins=None, groupby_list=['group', 'uuid'], range_type='robust',
                                 scalars=['velocity_2d_mm', 'height_ave_mm', 'length_mm', 'dist_to_center_px']):
    '''
    create fingerprint dataframe from scalar_df and mean_df

    Args:
        scalar_df ([pandas.DataFrame]): scalar summary dataframe generated from scalars_to_dataframe
        mean_df ([pandas.DataFrame]): syllable mean dataframe from compute_behavioral_statistics
        bin_num ([int], optional): number of bins for the features. Defaults to None.
        groupby_list (list, optional): the list of levels the fingerprint dataframe should be grouped by. Defaults to ['group', 'uuid'].

    Returns:
        summary ([pandas.DataFrame]): fingerprint dataframe
        range_dict ([dict]): dictionary that hold min max values of the features
    '''
    # pivot mean_df to be groupby x syllable
    syll_summary = mean_df.pivot_table(index=groupby_list, values='usage', columns='syllable')
    syll_summary.columns = pd.MultiIndex.from_arrays([['MoSeq'] * syll_summary.shape[1], syll_summary.columns])
    min_p = syll_summary.min().min()
    max_p = syll_summary.max().max()
    
    ranges = scalar_df.reset_index(drop=True)[scalars].agg(['min', 'max', robust_min, robust_max])
    # add syllable ranges to this df
    ranges['MoSeq'] = [min_p, max_p, min_p, max_p]
    range_idx = ['min', 'max'] if range_type == 'full' else ['robust_min', 'robust_max']

    def bin_scalars(data: pd.Series, n_bins=50, range_type='full'):
        _range = ranges.loc[range_idx, data.name]
        bins = np.linspace(_range.iloc[0], _range.iloc[1], n_bins)

        binned_data = data.value_counts(normalize=True, sort=False, bins=bins)
        binned_data = binned_data.sort_index().reset_index(drop=True)
        binned_data.index.name = 'bin'
        return binned_data

    # use total number of syllables 
    if n_bins is None:
        n_bins = syll_summary.shape[1] + 1 # num of bins (default to match the total number of syllables)

    binned_scalars = scalar_df.groupby(groupby_list)[scalars].apply(_apply_to_col, fn=bin_scalars, range_type=range_type, n_bins=n_bins)

    scalar_fingerprint = binned_scalars.pivot_table(index=groupby_list, columns='bin', values=binned_scalars.columns)

    fingerprints = scalar_fingerprint.join(syll_summary, how='outer')

    # rescale velocity - TODO: should velocity rescaling go somewhere else?
    vel_cols = [c for c in scalars if 'velocity' in c]
    if len(vel_cols) > 0:
        ranges[vel_cols] *= 30

    return fingerprints, ranges.loc[range_idx]


def plotting_fingerprint(summary, range_dict, preprocessor=None, num_level = 1, level_names = ['Group'], vmin=0, vmax = 0.2,
                         plot_columns=['dist_to_center_px', 'velocity_2d_mm', 'height_ave_mm', 'length_mm', 'MoSeq'],
                         col_names=[('Position','Dist. from center (px)'), ('Speed', 'Speed (mm/s)'), ('Height', 'Height (mm)'), ('Length', 'Length (mm)'), ('MoSeq','Syllable ID')]):
    '''
    plot the fingerprint heatmap

    Args:
        summary (pd.DataFrame): fingerprint dataframe
        range_dict (pd.DataFrame): pd.DataFrame that hold min max values of the features
        preprocessor (sklearn.preprocessing object, optional): Scalar for scaling the data by session. Defaults to None.
        num_level (int, optional): the number of groupby levels. Defaults to 1.
        level_names (list, optional): list of names of the levels. Defaults to ['Group'].
        vmin (int, optional): min value the figure color map covers. Defaults to 0.
        vmax (float, optional): max value the figure color map covers. Defaults to 0.2.
        plot_columns (list, optional): columns to plot
        col_names = (list, optional): list of (column name, x label) pairs
 
    Raises:
        Exception: num_levels greater than the existing levels
    '''
    # ensure number of groups is not over the number of available levels
    if num_level > len(summary.index.names):
        raise Exception('Too many levels to unpack. num_level should be less than', len(summary.index.names))

    name_map = dict(zip(plot_columns, col_names))
    
    levels = []
    level_plot = []
    level_ticks = []
    for i in range(num_level):
        level = summary.index.get_level_values(i)
        level_label = LabelEncoder().fit_transform(level)
        find_mid = (np.diff(np.r_[0,np.argwhere(np.diff(level_label)).ravel(),len(level_label)])/2).astype('int32')
        # store level value
        levels.append(level)
        level_plot.append(level_label)
        level_ticks.append(np.r_[0,np.argwhere(np.diff(level_label)).ravel()] + find_mid)
    
    # col_num = number of grouping/level + column in summary
    col_num = num_level + len(plot_columns)

    # https://matplotlib.org/stable/tutorials/intermediate/gridspec.html
    fig = plt.figure(1, figsize=(20, 18), facecolor='white')

    gs = GridSpec(2, col_num, wspace=0.1, hspace=0.1,
              width_ratios=[1]*num_level+[8]*(col_num-num_level), height_ratios = [10,0.1], figure=fig)

    # plot the level(s)
    for i in range(num_level):
        temp_ax = fig.add_subplot(gs[0,i])
        temp_ax.set_title(level_names[i], fontsize=20)
        temp_ax.imshow(level_plot[i][:,np.newaxis], aspect = 'auto', cmap = 'Set3')
        plt.yticks(level_ticks[i], levels[i][level_ticks[i]], fontsize=20)
        
        temp_ax.get_xaxis().set_ticks([])
    
    # plot the data
    for i, col in enumerate(plot_columns):
        name = name_map[col]
        temp_ax = fig.add_subplot(gs[0, i + num_level])
        temp_ax.set_title(name[0], fontsize=20)
        data = summary[col].to_numpy()
        if preprocessor is not None:
            data = preprocessor.fit_transform(data.T).T
            # reset vmin, vmax
            vmin, vmax = 0, 1
        # top to bottom is 0-20 for y axis
        if col == 'MoSeq':
            extent = [summary[col].columns[0], summary[col].columns[-1], len(summary) - 1, 0]
        else:
            extent = [range_dict[col].iloc[0], range_dict[col].iloc[1], len(summary) - 1, 0]

        pc = temp_ax.imshow(data, aspect='auto', interpolation='none', vmin=vmin, vmax=vmax, extent=extent)
        temp_ax.set_xlabel(name[1], fontsize=10)
        temp_ax.set_xticks(np.linspace(np.ceil(extent[0]), np.floor(extent[1]), 6).astype(int))
        # https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame
        temp_ax.set_yticks([])
        temp_ax.axis = 'tight'
    
    # plot colorbar
    cb = fig.add_subplot(gs[1,-1])
    plt.colorbar(pc, cax=cb, orientation='horizontal')

    # specify labels for feature scaling
    if preprocessor:
        cb.set_xlabel('Min Max')
    else:
        cb.set_xlabel('Percentage Usage')


def classifier_fingerprint(summary, features=['MoSeq'], preprocessor=None, classes=['group'], param_search=True, C_list=None,
                           model_type='lr', cv='loo', n_splits=5):
    '''
    classifier using the fingerprint dataframe

    Args:
        summary ([pandas.DataFrame]): fingerprint dataframe
        features (list, optional): Features for the classifier. ['MoSeq'] for MoSeq syllables or a list of MoSeq scalar values. Defaults to ['MoSeq'].
        preprocessor (sklearn.preprocessing object, optional): Scalar for scaling the data by feature. Defaults to None.
        target (list, optional): labels the classifier predicts. Defaults to ['group'].
        param_search (bool, optional): run GridSearchCV to find the regularization param for classifier. Defaults to True.
        C_list ([type], optional): list of C regularization paramters to search through. Defaults to None. If None, C_list will search through np.logspace(-6,3, 50)
        model_type (str, optional): name of the linear classifier. 'lr' for logistic regression or 'svc' for linearSVC. Defaults to 'lr'.
        cv (str, optional): cross validation type. 'loo' for LeaveOneOut 'skf' for StratifiedKFold. Defaults to 'loo'.
        n_splits (int, optional): number of splits for StratifiedKFold. Defaults to 5.

    Returns:
        y_true ([np.array]): array for true label
        y_pred ([np.array]): array for predicted label
        real_f1 ([np.array]): array for f1 score
        true_coef ([np.array]): array for model weights
        y_shuffle_true ([np.array]): array for shffuled label
        y_shuffle_pred ([np.array]): array for shuffled predicted label
        shuffle_f1 ([np.array]): array for shuffled f1 score
        shuffle_coef ([np.array]): array for shuffled model weights
    '''
    # set up data for classifier
    X = summary[features].to_numpy()
    print(X.shape)

    summary = summary.reset_index()
    y = summary[classes].squeeze()

    # set up model type
    Model = LogisticRegression if model_type == 'lr' else LinearSVC
    clf = Model(multi_class='ovr')

    if param_search:
        if C_list is None:
            C_list=np.logspace(-6,3, 50)

        parameters = {'C': C_list}
        grid_search = GridSearchCV(clf, parameters, cv=RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=5), scoring='accuracy')
        grid_search.fit(X,y)
        # set the best parameter for the classifier
        clf = clf.set_params(**grid_search.best_params_)
        print('classifier', clf)

    if isinstance(cv, str):
        if cv == 'loo':
            cv = LeaveOneOut()
        elif cv == 'skf':
            cv = StratifiedKFold(n_splits=n_splits)
    elif not hasattr(cv, 'split'):
        # user has supplied their own cv object
        raise ValueError('cv must either be a string containing "loo" or "skf" or a cross validation object')
    
    out = defaultdict(list)

    for split, (train_ix, test_ix) in enumerate(cv.split(X, y)):
        # split data
        X_train, X_test = X[train_ix], X[test_ix]
        y_train, y_test = y[train_ix], y[test_ix]
        # feature scalar
        if preprocessor is not None:
            preprocessor = clone(preprocessor)
            X_train = preprocessor.fit_transform(X_train)
            X_test = preprocessor.transform(X_test)

        # fit model on real data
        clf = clone(clf) # reset model to prevent data leakage
        clf.fit(X_train, y_train)
        # evaluate model
        out['y_true'].append(y_test)

        y_hat = clf.predict(X_test)
        out['y_pred'].append(y_hat)

        out['coefs'].append(clf.coef_[0])

    for i in range(100):
        y_shuffle = np.random.permutation(y)
        for split, (train_ix, test_ix) in enumerate(cv.split(X, y)):
            X_train, X_test = X[train_ix], X[test_ix]
            # shuffle y for shuffle analysis
            y_shuffle_train, y_shuffle_test = y_shuffle[train_ix], y_shuffle[test_ix]

            if preprocessor is not None:
                preprocessor = clone(preprocessor)
                X_train = preprocessor.fit_transform(X_train)
                X_test = preprocessor.transform(X_test)

            # shuffle data
            clf = clone(clf)
            clf.fit(X_train, y_shuffle_train)
            # evaluate 
            out['shuff_y_true'].append(y_shuffle_test)
            y_hat = clf.predict(X_test)
            out['shuff_y_pred'].append(y_hat)
            out['shuff_coefs'].append(clf.coef_[0])
            out['shuff_split'].append(split)

    out = {k: np.concatenate(v) if 'y' in k else np.array(v) for k, v in out.items()}
    out['accuracy'] = accuracy_score(out['y_true'], out['y_pred'])
    out['shuff_accuracy'] = accuracy_score(out['shuff_y_true'], out['shuff_y_pred'])

    return out


def _plot_cm(y_true, y_pred, ax, ax_labels, title):
    cm = confusion_matrix(y_true, y_pred)
    cm = cm / cm.sum(axis=1, keepdims=True)
    im = ax.imshow(cm, cmap='binary_r', vmin=0, vmax=1)
    plt.xticks(range(len(ax_labels)), ax_labels)
    plt.yticks(range(len(ax_labels)), ax_labels)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Real')
    ax.set_title(title)
    return im


def plot_cm(y_true, y_pred, y_shuffle_true, y_shuffle_pred):
    '''
    plot confusion matrix

    Args:
        y_true ([np.array]): array for true label
        y_pred ([np.array]): array for predicted label
        y_shuffle_true ([np.array]): array for shffuled label
        y_shuffle_pred ([np.array]): array for shuffled predicted label
    '''
    fig = plt.figure(figsize=(23, 10), facecolor='white')
    gs = GridSpec(ncols=3, nrows=1, wspace=0.1, figure = fig, width_ratios=[10,10,0.3])
    fig_ax = fig.add_subplot(gs[0,0])
    labels = np.unique(y_true)
    _plot_cm(y_true, y_pred, fig_ax, labels, f'Real Accuracy {accuracy_score(y_true, y_pred):0.2f}')

    fig_ax = fig.add_subplot(gs[0,1])
    im = _plot_cm(y_shuffle_true, y_shuffle_pred, fig_ax, labels, f'Shuffle Accuracy {accuracy_score(y_shuffle_true, y_shuffle_pred):0.2f}')
    fig_ax.set_ylabel('')
    fig_ax.set_yticklabels([])

    # plot colorbar
    cb = fig.add_subplot(gs[0,2])
    fig.colorbar(mappable=im, cax=cb, label='Fraction of labels', )
    fig.tight_layout()
    plt.show()